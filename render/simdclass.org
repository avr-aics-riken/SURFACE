#ifndef __SIMDCLASS_H__
#define __SIMDCLASS_H__

#if defined(__SSE2__) || (_M_IX86_FP >= 2) // _M_IX86_FP 2 = VS /arch:SSE2

// SIMD class
#include <emmintrin.h>

namespace lsgl {
namespace render {

static inline __m128d rcpe(__m128d a)
{ return _mm_cvtps_pd(_mm_rcp_ps(_mm_cvtpd_ps(a))); }


// 1/a with 53-bits precision.
static inline __m128d rcp(__m128d a)
{
  __m128d b = rcpe(a);
  b = _mm_sub_pd(_mm_add_pd(b, b), _mm_mul_pd(_mm_mul_pd(a, b), b));
  b = _mm_sub_pd(_mm_add_pd(b, b), _mm_mul_pd(_mm_mul_pd(a, b), b));
  b = _mm_sub_pd(_mm_add_pd(b, b), _mm_mul_pd(_mm_mul_pd(a, b), b));
  return b;
}

// 1.0f / a with 24-bits precision
inline __m128 rcp(__m128 a)
{
  __m128 b  = _mm_rcp_ps(a);
  // (b+b) - a*b*b
  b = _mm_sub_ps(_mm_add_ps(b, b), _mm_mul_ps(_mm_mul_ps(a, b), b));
  b = _mm_sub_ps(_mm_add_ps(b, b), _mm_mul_ps(_mm_mul_ps(a, b), b));
  return b;
}

struct __vec4_f {

  __vec4_f() { }

  __vec4_f(float v0, float v1, float v2, float v3) {
    u.v0 = _mm_setr_ps(v0, v1, v2, v3);
  }

  __vec4_f(__m128 v0) {
    u.v0 = v0;
  }

  __vec4_f(float v0) {
    u.v0 = _mm_set1_ps(v0);
  }

  __vec4_f(const __vec4_f &rhs) {
    u.v0 = rhs.u.v0;
  }

  __vec4_f operator=(const __vec4_f& rhs) {
    u.v0 = rhs.u.v0;
    return (*this);
  }

  __vec4_f operator+(const __vec4_f& rhs) {
    return __vec4_f(_mm_add_ps(u.v0, rhs.u.v0));
  }

  __vec4_f operator-(const __vec4_f& rhs) {
    return __vec4_f(_mm_sub_ps(u.v0, rhs.u.v0));
  }
  
  __vec4_f operator*(const __vec4_f& rhs) {
    return __vec4_f(_mm_mul_ps(u.v0, rhs.u.v0));
  }

  // @todo { Uew Newton-Raphson }
  __vec4_f operator/(const __vec4_f& rhs) {
    return __vec4_f(_mm_mul_ps(u.v0, rcp(rhs.u.v0)));
  }
  
  union {
    float v[4];
    struct { __m128 v0; };
  } u;
};

inline __vec4_f operator*(float f, const __vec4_f& rhs) {
  return __vec4_f(f) * rhs;
}

// --

struct __vec4_d {

  __vec4_d() { }

  __vec4_d(double v0, double v1, double v2, double v3) {
    u.v0 = _mm_setr_pd(v0, v1);
    u.v1 = _mm_setr_pd(v2, v3);
  }

  __vec4_d(__m128d v0, __m128d v1) {
    u.v0 = v0;
    u.v1 = v1;
  }

  __vec4_d(double v0) {
    u.v0 = _mm_setr_pd(v0, v0);
    u.v1 = _mm_setr_pd(v0, v0);
  }

  __vec4_d(const __vec4_d &rhs) {
    u.v0 = rhs.u.v0;
    u.v1 = rhs.u.v1;
  }

  __vec4_d operator=(const __vec4_d& rhs) {
    u.v0 = rhs.u.v0;
    u.v1 = rhs.u.v1;
    return (*this);
  }

  __vec4_d operator+(const __vec4_d& rhs) {
    return __vec4_d(_mm_add_pd(u.v0, rhs.u.v0),
                    _mm_add_pd(u.v1, rhs.u.v1));
  }

  __vec4_d operator-(const __vec4_d& rhs) {
    return __vec4_d(_mm_sub_pd(u.v0, rhs.u.v0),
                    _mm_sub_pd(u.v1, rhs.u.v1));
  }
  
  __vec4_d operator*(const __vec4_d& rhs) {
    return __vec4_d(_mm_mul_pd(u.v0, rhs.u.v0),
                    _mm_mul_pd(u.v1, rhs.u.v1));
  }

  // @todo { Uew Newton-Raphson }
  __vec4_d operator/(const __vec4_d& rhs) {
    return __vec4_d(_mm_mul_pd(u.v0, rcp(rhs.u.v0)),
                    _mm_mul_pd(u.v1, rcp(rhs.u.v1)));
  }
  
  union {
    double v[4];
    struct { __m128d v0, v1; };
  } u;
};

inline __vec4_d operator*(double f, const __vec4_d& rhs) {
  return __vec4_d(f) * rhs;
}

#endif

} // namespace render
} // namespace lsgl

#endif  // __SIMDCLASS_H__
